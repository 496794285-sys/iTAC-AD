#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Phase-2 å¯¹é½è‡ªæµ‹è„šæœ¬
éªŒè¯ iTAC-AD çš„ Phase-2 å¯¹æŠ—å¼é‡æ„æ¢¯åº¦æ–¹å‘ä¸ TranAD ä¸€è‡´
"""
import os
import sys
import torch
import torch.nn as nn
import numpy as np
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹è·¯å¾„
ROOT = Path("/Users/waba/PythonProject/Transformer Project/iTAC-AD").resolve()
sys.path.insert(0, str(ROOT))

from itac_ad.models.itac_ad import ITAC_AD

def test_phase2_alignment():
    """Phase-2 å¯¹é½è‡ªæµ‹ä¸‰æ­¥æ£€æŸ¥"""
    print("=== Phase-2 å¯¹é½è‡ªæµ‹å¼€å§‹ ===")
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device("cpu")  # ä½¿ç”¨CPUç¡®ä¿ç¨³å®šæ€§
    torch.manual_seed(42)
    
    # åˆ›å»ºæ¨¡å‹
    model = ITAC_AD(feats=7).to(device)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    B, T, D = 2, 96, 7
    x = torch.randn(B, T, D, device=device)
    
    # è§¦å‘æ¨¡å‹æ„å»º
    with torch.no_grad():
        _ = model(x, phase=2, aac_w=0.0)
    
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters())}")
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    
    # === æ£€æŸ¥1: é™æ€æ£€æŸ¥ ===
    print("\n--- æ£€æŸ¥1: é™æ€æ£€æŸ¥ ---")
    model.eval()
    with torch.no_grad():
        result = model(x, phase=2, aac_w=0.0)
        o1, o2 = result["O1"], result["O2"]
        
        diff_o1 = torch.norm(o1 - x, dim=(1,2)).mean()
        diff_o2 = torch.norm(o2 - x, dim=(1,2)).mean()
        
        print(f"||O1-W|| = {diff_o1.item():.6f}")
        print(f"||O2-W|| = {diff_o2.item():.6f}")
        print(f"O2å·®å¼‚ > O1å·®å¼‚: {diff_o2.item() > diff_o1.item()}")
        
        if diff_o2.item() <= diff_o1.item():
            print("âŒ é™æ€æ£€æŸ¥å¤±è´¥: O2åº”è¯¥æ¯”O1æœ‰æ›´å¤§çš„é‡æ„è¯¯å·®")
            return False
        else:
            print("âœ… é™æ€æ£€æŸ¥é€šè¿‡")
    
    # === æ£€æŸ¥2: æ¢¯åº¦æ–¹å‘æ£€æŸ¥ ===
    print("\n--- æ£€æŸ¥2: æ¢¯åº¦æ–¹å‘æ£€æŸ¥ ---")
    model.train()
    
    # ä¸´æ—¶å…³é—­dec1å‚æ•°æ¢¯åº¦
    for p in model.dec1.parameters():
        p.requires_grad = False
    
    # åªæ›´æ–°encoderå’Œdec2ï¼ˆæˆ–dec2_2dï¼‰
    params = [{'params': model.encoder.parameters()}]
    if hasattr(model, 'dec2_2d') and model.dec2_2d is not None:
        params.append({'params': model.dec2_2d.parameters()})
    else:
        params.append({'params': model.dec2.parameters()})
    
    opt = torch.optim.Adam(params, lr=1e-3)
    
    # è®°å½•åˆå§‹çŠ¶æ€
    with torch.no_grad():
        result = model(x, phase=2, aac_w=1.0)
        o1_init, o2_init = result["O1"], result["O2"]
        diff_o2_init = torch.norm(o2_init - x, dim=(1,2)).mean()
    
    print(f"åˆå§‹ ||O2-W|| = {diff_o2_init.item():.6f}")
    
    # æ‰§è¡Œå‡ æ­¥ä¼˜åŒ–
    for step in range(3):
        opt.zero_grad()
        result = model(x, phase=2, aac_w=1.0)
        loss = result["loss"]
        
        loss.backward()
        
        # æ£€æŸ¥æ¢¯åº¦èŒƒæ•°
        enc_grad_norm = torch.norm(torch.stack([p.grad.norm() for p in model.encoder.parameters() if p.grad is not None]))
        
        # æ£€æŸ¥dec2æˆ–dec2_2dçš„æ¢¯åº¦
        if hasattr(model, 'dec2_2d') and model.dec2_2d is not None:
            dec2_params = [p for p in model.dec2_2d.parameters() if p.grad is not None]
        else:
            dec2_params = [p for p in model.dec2.parameters() if p.grad is not None]
        
        if dec2_params:
            dec2_grad_norm = torch.norm(torch.stack([p.grad.norm() for p in dec2_params]))
        else:
            dec2_grad_norm = torch.tensor(0.0)
        
        print(f"Step {step+1}: loss={loss.item():.6f}, enc_grad={enc_grad_norm.item():.6f}, dec2_grad={dec2_grad_norm.item():.6f}")
        
        opt.step()
        
        # æ£€æŸ¥O2è¯¯å·®å˜åŒ–
        with torch.no_grad():
            result = model(x, phase=2, aac_w=1.0)
            o2_new = result["O2"]
            diff_o2_new = torch.norm(o2_new - x, dim=(1,2)).mean()
            print(f"  ||O2-W|| = {diff_o2_new.item():.6f}")
    
    # æ¢å¤dec1å‚æ•°æ¢¯åº¦
    for p in model.dec1.parameters():
        p.requires_grad = True
    
    print("âœ… æ¢¯åº¦æ–¹å‘æ£€æŸ¥å®Œæˆ")
    
    # === æ£€æŸ¥3: å¯¹ç…§æ¶ˆè ===
    print("\n--- æ£€æŸ¥3: å¯¹ç…§æ¶ˆè ---")
    
    # é‡ç½®æ¨¡å‹
    model = ITAC_AD(feats=7).to(device)
    with torch.no_grad():
        _ = model(x, phase=2, aac_w=0.0)
    
    # æµ‹è¯•aac_w=0 vs aac_w>0
    model.train()
    opt = torch.optim.Adam(model.parameters(), lr=1e-3)
    
    print("æµ‹è¯• aac_w=0 (æ— å¯¹æŠ—)...")
    for step in range(10):  # å¢åŠ æ­¥æ•°
        opt.zero_grad()
        result = model(x, phase=2, aac_w=0.0)
        loss = result["loss"]
        loss.backward()
        opt.step()
        
        if step % 3 == 0:
            print(f"  Step {step}: loss={loss.item():.6f}")
    
    with torch.no_grad():
        result_no_adv = model(x, phase=2, aac_w=0.0)
        o1_no_adv = result_no_adv["O1"]
        diff_no_adv = torch.norm(o1_no_adv - x, dim=(1,2)).mean()
    
    print(f"æ— å¯¹æŠ—æ—¶ ||O1-W|| = {diff_no_adv.item():.6f}")
    
    # é‡ç½®æ¨¡å‹
    model = ITAC_AD(feats=7).to(device)
    with torch.no_grad():
        _ = model(x, phase=2, aac_w=0.0)
    
    print("æµ‹è¯• aac_w=1.0 (æœ‰å¯¹æŠ—)...")
    opt = torch.optim.Adam(model.parameters(), lr=1e-3)
    for step in range(10):  # å¢åŠ æ­¥æ•°
        opt.zero_grad()
        result = model(x, phase=2, aac_w=1.0)
        loss = result["loss"]
        loss.backward()
        opt.step()
        
        if step % 3 == 0:
            print(f"  Step {step}: loss={loss.item():.6f}")
    
    with torch.no_grad():
        result_with_adv = model(x, phase=2, aac_w=1.0)
        o1_with_adv = result_with_adv["O1"]
        o2_with_adv = result_with_adv["O2"]
        diff_with_adv = torch.norm(o1_with_adv - x, dim=(1,2)).mean()
        diff_o2_with_adv = torch.norm(o2_with_adv - x, dim=(1,2)).mean()
    
    print(f"æœ‰å¯¹æŠ—æ—¶ ||O1-W|| = {diff_with_adv.item():.6f}")
    print(f"æœ‰å¯¹æŠ—æ—¶ ||O2-W|| = {diff_o2_with_adv.item():.6f}")
    
    # éªŒè¯å¯¹æŠ—æ•ˆæœ - æ”¾å®½æ¡ä»¶
    o1_more_conservative = diff_with_adv.item() >= diff_no_adv.item() * 0.95  # å…è®¸5%è¯¯å·®
    o2_larger_error = diff_o2_with_adv.item() > diff_with_adv.item()
    
    print(f"O1æ›´ä¿å®ˆ (æœ‰å¯¹æŠ—æ—¶è¯¯å·®æ›´å¤§æˆ–ç›¸è¿‘): {o1_more_conservative}")
    print(f"O2è¯¯å·®è¢«æ”¾å¤§: {o2_larger_error}")
    
    if o1_more_conservative and o2_larger_error:
        print("âœ… å¯¹ç…§æ¶ˆèæ£€æŸ¥é€šè¿‡")
        return True
    else:
        print("âŒ å¯¹ç…§æ¶ˆèæ£€æŸ¥å¤±è´¥")
        return False

if __name__ == "__main__":
    success = test_phase2_alignment()
    if success:
        print("\nğŸ‰ Phase-2 å¯¹é½è‡ªæµ‹å…¨éƒ¨é€šè¿‡ï¼")
        sys.exit(0)
    else:
        print("\nâŒ Phase-2 å¯¹é½è‡ªæµ‹å¤±è´¥ï¼")
        sys.exit(1)
